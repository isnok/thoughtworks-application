<h2 id="how-to-run">How to run</h2>
<p>Setting up the environments requires virtualbox, vagrant and ansible installed. Run <code>vagrant up</code> in the <code>training_environment</code> directory to bring up the training environment. Once started, the application can be accessed at:</p>
<ul>
<li><a href="http://localhost:8080" class="uri">http://localhost:8080</a> (training environment - application)</li>
<li><a href="http://localhost:8080/manager/html/" class="uri">http://localhost:8080/manager/html/</a> (credentials are tomcat_admin:tomcat)</li>
</ul>
<p>Run <code>vagrant up</code> in the directory <code>production_environment</code>, to bring up the following:</p>
<ul>
<li><a href="http://localhost:8081" class="uri">http://localhost:8081</a> (production environment - balancer1)</li>
<li><a href="http://localhost:8082" class="uri">http://localhost:8082</a> (production environment - balancer2)</li>
<li><a href="http://localhost:8081/haproxy?stats" class="uri">http://localhost:8081/haproxy?stats</a> (haproxy_stats_admin:iePha4se)</li>
</ul>
<h2 id="architecture">Architecture</h2>
<p>All systems are based on the vagrant image <code>bento/debian-8.5</code>, for more information visit <a href="https://atlas.hashicorp.com/bento/boxes/debian-8.5" class="uri">https://atlas.hashicorp.com/bento/boxes/debian-8.5</a>. To serve the war file in each environment, I choose tomcat because it is an apache project. In the training environment tomcat also serves the static files, so it is literally the only service defined there. In production the static files are served via gatling (<a href="http://www.fefe.de/gatling/" class="uri">http://www.fefe.de/gatling/</a>), because it does the trick with least complexity; it could be replaced when the system complexity grows.</p>
<h2 id="ram">Ram</h2>
<p>The tomcat servers were configured to launch jvms with 64mb of ram (due to percepted memory usage and to make development easier). Also all vagrant boxes are configured to 256mb of ram in the production environment (128mb also seemed to work). In a real-world production setup one would surely increase the ram (prevayler requires to store the full application state in ram).</p>
<h2 id="production-environment">Production environment</h2>
<p>The production setup looks roughly as follows:</p>
<pre><code>    Internet --&gt; load balancers --&gt; appservers / staticservers
                    [haproxy]        [tomcat8] / [gatling]
                           |            |           |
                           v            v           v
                             logservers   [rsyslog]</code></pre>
<p>The haproxy load balancers route an incoming request to the appropriate backend (appservers/static) based on the request url. The internet-facing port(s) of the environment are forwarded to the vagrant host's ports.</p>
<p>The hosts in the provided setup are (all ports <code>tcp</code>):</p>
<ul>
<li>balancer1 (ip: 10.0.0.10, port 80 forwarded to vagrant host port 8081)</li>
<li>balancer2 (ip: 10.0.0.11, port 80 forwarded to vagrant host port 8082)</li>
<li>appserver1 (ip: 10.0.0.20, serves the application at port 8080)</li>
<li>appserver2 (ip: 10.0.0.21, serves the application at port 8080)</li>
<li>static1 (ip: 10.0.0.30, serves static content at port 8090)</li>
<li>static1 (ip: 10.0.0.31, serves static content at port 8090)</li>
<li>logserver1 (ip: 10.0.0.100, receives logs from balancers, appservers and statics at port: 514)</li>
<li>logserver2 (ip: 10.0.0.101, receives logs from balancers, appservers and statics at port: 514)</li>
</ul>
<p>The configurations are generated to provide one-node fault-tolerance for the whole system.</p>
<p>All internal IP adresses used in the production setup are defined in <code>production-config.yml</code>. The variables defined in <code>production-config.yml</code> are used in ansible as well as vagrant and provide a single point of configuration for adding more machines to the environment.</p>
<p>Adding new hosts:</p>
<ol>
<li>Add section to <code>production-config.yml</code></li>
<li>Vagrant up the machine</li>
<li>If an application or static server was added, it is not yet included in the load balancer configs. Reprovision them with <code>vagrant provision balancerN</code> to deploy a new version of the haproxy config and restart haproxy.</li>
</ol>
<h2 id="discussion-of-the-application">Discussion of the application</h2>
<p>The current implementation (Prevayler) does not provide a way to synchronize application data between network nodes. This makes one-node fault-tolerance and keeping tight SLAs very hard. I built a setup, that runs two appserver nodes, and their state diverges, once the first &quot;news&quot; is entered. Synchronizing the prevayler data directory nodes does not trigger updates in &quot;the other appserver&quot;. Thus, I would recommend the dev-team to move away from prevayler, towards a distributed database, that fits the data flow of the application. Also, not having an authentication system for content creation in place, seems like a bad idea to me. The war file also seems to statically reference the directory <code>/Users/dcameron/persistence/files*</code> as the persistence directory. I would advise to make that configurable, for example through an environment variable.</p>
<h2 id="discussion-of-the-environment">Discussion of the environment</h2>
<ul>
<li>service configurations are all very basic</li>
<li>no encryption configured (https / deployment keys / ssl tunnels for logs)</li>
<li>no backup of application data / disaster recovery plan</li>
<li>vagrant adds support for other virtualization providers as well</li>
<li>the ansible playbooks could even be used to deploy to platforms not supported by vagrant</li>
</ul>
<h2 id="limited-release">Limited Release</h2>
<p>A hosting provider must be chosen from the market, and the scripts be adapted to set up the stack there. An alerting system should be installed. I would probably suggest using a cloud provider like uptimerobot (<a href="http://uptimerobot.com" class="uri">http://uptimerobot.com</a>). Ideally the most common administrative processes (restarting servers/applications) should be automated. The system should also have a resource monitoring solution in place to study bottlenecks and load patterns in the limited release phase.</p>
<h2 id="global-release">Global Release</h2>
<p>World-wide sub-second response-times require a distribution network around the world. For static content a CDN can be used, the application logic would be the part to focus on.</p>
